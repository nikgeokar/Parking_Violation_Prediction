{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model is trained in this Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Proccesing\n",
    "import folium\n",
    "import pandas as pd\n",
    "#Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Dense,add\n",
    "from tensorflow.keras.models import Model\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "Project_Path='Local Path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I load the Dataset 'Legal_Illegal', the weather file 'Final_Weather_Data', and the distance domain file from the 19 points of interest 'Distance_Data'. I split the Dataset into 80% train-set and 20% test-set and then I save them in separate files so I always have the same train-set and test-set to know if each change helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_Data=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "Legal_illegal=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "\n",
    "train_data,test_data = train_test_split(Legal_illegal,test_size=0.2,random_state=42)\n",
    "train_data.to_csv(Project_Path+ '/Data/Train.csv')\n",
    "test_data.to_csv(Project_Path+ '/Data/Test.csv')\n",
    "\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are functions used throughout the file during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function I normalize the values to an interval of 0.1-0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(Targets):\n",
    "    Targets = (((0.9-0.1) * (Targets - 0)) / (1 - 0)) + 0.1\n",
    "    return Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function I do the inverse normalization of the \"Normalization\" function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inverse_Normalization(Targets):\n",
    "    Targets=(Targets-0.1)/0.8\n",
    "    return Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function I calculate the actual mae of normalized predictions and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true, y_pred):\n",
    "    y_true=Inverse_Normalization(y_true)\n",
    "    y_pred=Inverse_Normalization(y_pred)\n",
    "    MAE=tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function I calculate the true mse of normalized predictions and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    y_true=Inverse_Normalization(y_true)\n",
    "    y_pred=Inverse_Normalization(y_pred)\n",
    "    MSE=tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    y_true=Inverse_Normalization(y_true)\n",
    "    y_pred=Inverse_Normalization(y_pred)\n",
    "    MAPE=tf.keras.metrics.mean_absolute_percentage_error (y_true, y_pred)\n",
    "    return MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function accepts train-set and test-set. It applies standardization to them and returns them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller(Train,Test):\n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Scalled_Train_data=Standar_Scaller.fit_transform(Train)\n",
    "    Scalled_val_data = Standar_Scaller.transform(Test)\n",
    "    return Scalled_Train_data,Scalled_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I declare Initializers for the hidden layer kernels and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kernel_Sigmoid_Initializer = tf.keras.initializers.GlorotUniform()\n",
    "Kernel_Relu_Initializer =tf.keras.initializers.HeUniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function constructs Residual Neural Networks. Accepts the attribute size to use as input size. Accepts a variable that will be used to initialize the output level bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residual_NN(Input_Shape,Bias_Sigmoid_Initializer):\n",
    "    Input_Layer = Input(shape=(Input_Shape,))\n",
    "    Dense_Layer1 = Dense(512, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Input_Layer)\n",
    "    Dense_Layer2 = Dense(256, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer1)\n",
    "    Dense_Layer3 = Dense(128, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer2)\n",
    "    Dense_Layer4 = Dense(64, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer3)\n",
    "    Dense_Layer5 = Dense(128, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer4)\n",
    "    Residual_Add = add([Dense_Layer3, Dense_Layer5])\n",
    "    Dense_Layer6 = Dense(32, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Residual_Add)\n",
    "    Output_Layer = Dense(1, activation='sigmoid',kernel_initializer=Kernel_Sigmoid_Initializer,bias_initializer=Bias_Sigmoid_Initializer)(Dense_Layer6)\n",
    "\n",
    "    model2 = Model(inputs=Input_Layer, outputs=Output_Layer)\n",
    "    model2.compile(optimizer='adamax', loss=MSE, metrics=[MAE,MAPE])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I must mention that in the previous notebook I created the dataset I did not edit it to the final form for training. For the reason that the dataset I created in the previous notebook had to have some features that will not be used in training but will be used to apply the smoothing data augmentation technique to it later. For now I will not implement this technique but will train our model without it. To do this I need to make some changes to the format of the dataset I created in the previous notebook so that it is ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function prepares the dataset so that it is ready for training. First it deletes an unneeded feature. Then it changes the order of some columns and then adds the distances from the points of interest and the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Dataset): \n",
    "    Dataset=Dataset.drop(['Time_Int'], axis=1)\n",
    "    a=Dataset['Slot_Timeint']\n",
    "    b=Dataset['Ilegality_Rate']\n",
    "    Dataset=Dataset.drop(['Slot_Timeint'], axis=1)\n",
    "    Dataset=Dataset.drop(['Ilegality_Rate'], axis=1)\n",
    "    Dataset.insert(8, \"Real_Time\", a, True)\n",
    "    Dataset.insert(9, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Final_Weather_Data, on='Key')\n",
    "    Dataset=Dataset.drop(['Key'], axis=1)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Distance_Data, on='Slot_id')\n",
    "    Dataset=Dataset.drop(['Slot_id'], axis=1)\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below applies an exponential decay technique to the learning rate after epoch 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 40:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run a 'type' 4-fold cross validation procedure. Using only the training-set to find the best training time. At the same time I save the history of mse and mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "TrainDF=train_data\n",
    "k = 4\n",
    "num_epochs = 60\n",
    "\n",
    "all_mape_histories = []\n",
    "all_mae_histories = []\n",
    "all_loss_histories = []\n",
    "all_Tmae_histories = []\n",
    "all_Tloss_histories = []\n",
    "\n",
    "for i in tqdm(range(0,k)):\n",
    "        print('processing fold #', i)\n",
    "        TrainDF=shuffle(TrainDF) # Shuffles the training set\n",
    "        Train,Val = train_test_split(TrainDF,test_size=0.2,random_state=42) #Το χωρίζει σε 80%-20% \n",
    "        \n",
    "        Val=Prepare_Dataset(Val) #It calls the function to make the final changes to the dataset so that it is ready for training\n",
    "        Train=Prepare_Dataset(Train) #It calls the function to make the final changes to the dataset so that it is ready for training\n",
    "        \n",
    "        #It is divided into target and features\n",
    "        Final_Train_targets=Train['Real_Rate'] \n",
    "        Final_Train_data=Train.drop(['Real_Rate'], axis=1)\n",
    "        \n",
    "        Final_val_targets=Val['Real_Rate']\n",
    "        Final_val_data=Val.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "        #Calls the function to do standardization \n",
    "        Final_Train_data,Final_val_data=Scaller(Final_Train_data,Final_val_data)\n",
    "        \n",
    "        #It calls the function to do normalization (0.1-0.09) on the targets\n",
    "        Final_Train_targets = Normalization(Final_Train_targets)\n",
    "        Final_val_targets = Normalization(Final_val_targets)\n",
    "        \n",
    "        #Finds the mean of the targets and uses it to initialize the biases when the model is built  \n",
    "        Target=Final_Train_targets\n",
    "        Bias_Initial_Out=Target.mean()\n",
    "        Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "        \n",
    "        \n",
    "        #Builds the Keras model\n",
    "        model = Residual_NN(Input_Shape=Final_Train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "\n",
    "        history = model.fit(Final_Train_data, Final_Train_targets,\n",
    "                            validation_data=(Final_val_data, Final_val_targets),\n",
    "                            epochs=num_epochs,callbacks=callback,batch_size=16, verbose=1)\n",
    "        #Saves the results\n",
    "\n",
    "        #mape_history = history.history['val_MAPE']\n",
    "        mae_history = history.history['val_MAE']\n",
    "        loss_history = history.history['val_loss']\n",
    "        Tmae_history = history.history['MAE']\n",
    "        Tloss_history = history.history['loss']\n",
    "        \n",
    "        #all_mape_histories.append(mape_history)\n",
    "        all_mae_histories.append(mae_history)\n",
    "        all_loss_histories.append(loss_history)\n",
    "        all_Tmae_histories.append(Tmae_history)\n",
    "        all_Tloss_histories.append(Tloss_history)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I find the averages of the results per season from the four iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_loss_history = [\n",
    "    np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
    "average_Tmae_history = [\n",
    "    np.mean([x[i] for x in all_Tmae_histories]) for i in range(num_epochs)]\n",
    "average_Tloss_history = [\n",
    "    np.mean([x[i] for x in all_Tloss_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save them to external text files so I can have them and print out whatever chart I need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/No Smoothing/ValMae.txt', \"w\") as file:\n",
    "    file.write(str(average_mae_history))\n",
    "with open(Project_Path+ '/Results/No Smoothing/ValLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_loss_history))\n",
    "with open(Project_Path+ '/Results/No Smoothing/TrainMae.txt', \"w\") as file:\n",
    "    file.write(str(average_Tmae_history))\n",
    "with open(Project_Path+ '/Results/No Smoothing/TrainLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_Tloss_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the list containing the Validation mae to find the appropriate training season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/No Smoothing/ValMae.txt', \"r\") as file:\n",
    "    average_mae_history = eval(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am printing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.5):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.title('4-Fold Cross Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation_Mae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find what is the best season. That is, the one with the youngest Mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Mae=min(smooth_mae_history)\n",
    "Best_Epoch=smooth_mae_history.index(Min_Mae)\n",
    "Best_Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now knowing the best training time I train the model and check it with the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test.csv',sep=',',index_col=0)\n",
    "\n",
    "#Calls the function to make the final changes to the dataset so that it is ready for training\n",
    "TestDF=Prepare_Dataset(test_data)\n",
    "TrainDF=Prepare_Dataset(train_data)\n",
    "\n",
    "#It is divided into target and features\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "test_targets=TestDF['Real_Rate']\n",
    "test_data=TestDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "#Calls the function to make the final changes to the dataset so that it is ready for training\n",
    "test_targets = Normalization(test_targets)\n",
    "train_targets = Normalization(train_targets)\n",
    "\n",
    "#Finds the mean of the targets and uses it to initialize the biases when the model is built  \n",
    "Target=train_targets\n",
    "Bias_Initial_Out=Target.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "\n",
    "#Calls the function to do standardization \n",
    "train_data,test_data=Scaller(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been training for 44 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Residual_NN(Input_Shape=train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "model.fit(train_data,train_targets,callbacks=callback,\n",
    "          epochs=44, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score, test_mape_score = model.evaluate(test_data, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do the above procedure by applying smoothing to both the train-set and the validation/test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function accepts a dataset and returns a smoothed-dataset. Essentially using one data sample creates three, that is, it triples the data set. I have described the process in detail in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smouth(Legal_illegal):\n",
    "    #Takes the recorded time-slots and converts them so that they are displayed with their center \"eg: 7:00->7:30\"\n",
    "    Time_Slots=[21600,25200,28800,32400,36000,39600,43200,46800,50400,54000,57600,61200,64800,68400,72000]\n",
    "    Time_SlotsCenter=[]\n",
    "    for i in range (0,len(Time_Slots)):\n",
    "        Time_SlotsCenter.append((Time_Slots[i]+1800)/timedelta(days=1).total_seconds())\n",
    "    Time_Slots=Time_SlotsCenter\n",
    "\n",
    "    \n",
    "    Scan_List2=Legal_illegal.values.tolist()\n",
    "    NewData=[]\n",
    "    Slots=[]\n",
    "    for i in range(0,len(Scan_List2)):\n",
    "        Helper=[]\n",
    "        Helper2=[]\n",
    "        Rate=Scan_List2[i][6] #Actual rate of parking violations \n",
    "        Real_Time=Scan_List2[i][5] #Real time of the check \n",
    "\n",
    "        #It measures the distances from all time slots for each control\n",
    "        #By measuring the distance from the actual time of the control with the centers of the timeslots\n",
    "        #It finds the three closest time-slots for each check\n",
    "        Distances=[]\n",
    "        for j in range (0,len(Time_Slots)):\n",
    "            Distances.append(abs(Time_Slots[j]-Real_Time))\n",
    "        Slots=np.column_stack((Time_Slots, Distances))\n",
    "        Slots = sorted(Slots, key=lambda x: x[1])\n",
    "        Slot1,Slot2,Slot3=Slots[0][0],Slots[1][0],Slots[2][0] #3 nearest time-slots\n",
    "        D1,D2,D3=Slots[0][1],Slots[1][1],Slots[2][1] #Time Distances from the 3 nearest time-slots\n",
    "\n",
    "        #Creates a data sample with all the characteristics from the original setting the closest \"Slot1\" as a time slot\n",
    "        #Puts the actual 'Rate' as the delinquency rate\n",
    "        #Like a distance of 0, because after we put the actual percentage it is as if we consider it to be exactly at\n",
    "        #center of the time-slot. I explain the formula in more detail in the paper.\n",
    "        Helper=Scan_List2[i][:11]\n",
    "        Helper.append(Slot1)\n",
    "        Helper.append(Rate)\n",
    "        Helper.append(0)\n",
    "        NewData.append(Helper)\n",
    "        \n",
    "        #Creates a second data sample with all the attributes from the original one\n",
    "        #as the second closest time slot\n",
    "        #Puts as delinquency rate what is obtained by applying the Gaussian\n",
    "        #Sets the distance 'D2' from the second closest timeslot\n",
    "        Helper=Scan_List2[i][:11]\n",
    "        Helper.append(Slot2)\n",
    "        #Formula is \"-distance in minutes/210 minutes\"\n",
    "        #'0.14583' is 210 minutes, according to my time normalization\n",
    "        X1=(-D2/0.14583) \n",
    "        X2=np.exp(X1)\n",
    "        Helper.append(X2*Rate)\n",
    "        Helper.append(D2)\n",
    "        NewData.append(Helper)\n",
    "\n",
    "        #Creates a third data sample with all the characteristics from the original one by setting it as a time slot\n",
    "        #the third closest\n",
    "        #Puts as delinquency rate what is obtained by applying the Gaussian\n",
    "        #Sets the distance 'D3' from the third closest timeslot\n",
    "        Helper=Scan_List2[i][:11]\n",
    "        Helper.append(Slot3)\n",
    "        X1=(-D3/0.14583)\n",
    "        X2=np.exp(X1)\n",
    "        Helper.append(X2*Rate)\n",
    "        Helper.append(D3)\n",
    "        NewData.append(Helper)\n",
    "\n",
    "    Col=['Slot_id','Key','Date_Sin','Slot_Timeint','Covid','Time_Int','Ilegality_Rate','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','Time_Distance']\n",
    "    Legal_illegal = pd.DataFrame (NewData, columns = Col)\n",
    "    \n",
    "    \n",
    "    #Create the final dataset by deleting the features that are not needed.\n",
    "    #Repositions an attribute to be in the correct position.\n",
    "    Legal_illegal=Legal_illegal.drop(['Slot_Timeint'], axis=1)\n",
    "    Legal_illegal=Legal_illegal.drop(['Time_Int'], axis=1)\n",
    "    Legal_illegal=Legal_illegal.drop(['Ilegality_Rate'], axis=1)\n",
    "    a=Legal_illegal['Time_Distance']\n",
    "    Legal_illegal=Legal_illegal.drop(['Time_Distance'], axis=1)\n",
    "    Legal_illegal.insert(8, \"Time_Distance\", a, True)\n",
    "    return Legal_illegal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used one sample above to create 2 more and gave the same characteristics apart from the delinquency rate and the timeslot. We have also given a 'Key', the key is datetime and I use it afterwards to merge the hourly weather values. Since the 2 new samples represent another time the 'Key' must also be changed. In the following function we find the correct keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Weather(Legal_illegal):\n",
    "    #Converts time to normal format\n",
    "    Time=Legal_illegal['Real_Time']*timedelta(days=1).total_seconds()\n",
    "    Time=Time/3600\n",
    "    Time=Time.astype(int)\n",
    "    Time=Time.values.tolist()\n",
    "    NewT=[]\n",
    "    #If the time is 9:00, it converts it to 09:00 to have the same format as the weather key\n",
    "    for i in range (0,len(Time)):\n",
    "        Str=str(Time[i])\n",
    "        if Time[i]>=10:\n",
    "            NewT.append(Str)\n",
    "        else:\n",
    "            NewT.append('0'+Str)\n",
    "\n",
    "    #Puts \":00\" at the end of each hour to have the same format as the weather key\n",
    "    Time=pd.DataFrame(NewT,columns=[\"Hour\"])\n",
    "    Time= Time[\"Hour\"].map(str)+ ':00'\n",
    "    #Also takes the date\n",
    "    T_List=Legal_illegal.values.tolist()\n",
    "    Date=[]\n",
    "    for i in range (0,len(Legal_illegal)):\n",
    "        D,H=T_List[i][1].split(' ')\n",
    "        Date.append(D)\n",
    "\n",
    "    #Sets the date as a new key\n",
    "    Legal_illegal=Legal_illegal.drop(['Key'], axis=1)\n",
    "    Legal_illegal.insert(1, \"Key\", Date, True)\n",
    "    \n",
    "    #Adds the time to the new key\n",
    "    Key_Weather=Legal_illegal['Key'].map(str) + ' ' + Time\n",
    "    Legal_illegal=Legal_illegal.drop(['Key'], axis=1)\n",
    "    Legal_illegal.insert(1, \"Key\", Key_Weather, True)\n",
    "    \n",
    "    #Merge with the weather data\n",
    "    Legal_illegal=pd.merge(Legal_illegal, Final_Weather_Data, on='Key')\n",
    "    \n",
    "    Legal_illegal=Legal_illegal.drop(['Key'], axis=1)\n",
    "    return Legal_illegal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below merges the file from the field distances from the 19 points of interest and deletes the slot id because we don't use it in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Slot_Distances(Legal_illegal):\n",
    "    Legal_illegal['Slot_id'] = Legal_illegal['Slot_id'].astype(float)\n",
    "    Legal_illegal['Slot_id'] = Legal_illegal['Slot_id'].astype(int)\n",
    "    Legal_illegal=pd.merge(Legal_illegal, Distance_Data, on='Slot_id')\n",
    "    Legal_illegal=Legal_illegal.drop(['Slot_id'], axis=1)\n",
    "    return Legal_illegal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below essentially combines the three functions above and these constitute the smoothing process. Where having as input the data-set I created in the previous notebook I get the smoothed dataset as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apply_Smoothing(Legal_illegal):\n",
    "    Legal_illegal=Smouth(Legal_illegal)\n",
    "    Legal_illegal=Get_Weather(Legal_illegal)\n",
    "    Legal_illegal=Get_Slot_Distances(Legal_illegal)\n",
    "    return Legal_illegal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the files I use as train/test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below applies an exponential decay technique to the learning rate after epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the same process as I did before, just in each iteration I apply smoothing to both the validation and the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "TrainDF=train_data\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 60\n",
    "all_mae_histories = []\n",
    "all_loss_histories = []\n",
    "all_Tmae_histories = []\n",
    "all_Tloss_histories = []\n",
    "\n",
    "for i in tqdm(range(0,k)):\n",
    "        print('processing fold #', i)\n",
    "        TrainDF=shuffle(TrainDF)\n",
    "        Train,Val = train_test_split(TrainDF,test_size=0.2,random_state=42)\n",
    "        \n",
    "      \n",
    "        Val=Apply_Smoothing(Val) #smoothing\n",
    "        Train=Apply_Smoothing(Train) #smoothing\n",
    "        \n",
    "        Final_Train_targets=Train['Real_Rate']\n",
    "        Final_Train_data=Train.drop(['Real_Rate'], axis=1)\n",
    "        \n",
    "        Final_val_targets=Val['Real_Rate']\n",
    "        Final_val_data=Val.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "        \n",
    "        Final_Train_data,Final_val_data=Scaller(Final_Train_data,Final_val_data)\n",
    "        \n",
    "        Final_Train_targets = Normalization(Final_Train_targets)\n",
    "        Final_val_targets = Normalization(Final_val_targets)\n",
    "        \n",
    "        Target=Final_Train_targets\n",
    "        Target.append(Final_val_targets)\n",
    "        Bias_Initial_Out=Target.mean()\n",
    "        Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "        \n",
    "        \n",
    "        # Build the Keras model (already compiled)\n",
    "        model = Residual_NN(Input_Shape=Final_Train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = model.fit(Final_Train_data, Final_Train_targets,\n",
    "                            validation_data=(Final_val_data, Final_val_targets),\n",
    "                            epochs=num_epochs,callbacks=callback,batch_size=16, verbose=1)\n",
    "        mae_history = history.history['val_MAE']\n",
    "        loss_history = history.history['val_loss']\n",
    "        Tmae_history = history.history['MAE']\n",
    "        Tloss_history = history.history['loss']\n",
    "        all_mae_histories.append(mae_history)\n",
    "        all_loss_histories.append(loss_history)\n",
    "        all_Tmae_histories.append(Tmae_history)\n",
    "        all_Tloss_histories.append(Tloss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I find the averages of the results per season from the four iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_loss_history = [\n",
    "    np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
    "average_Tmae_history = [\n",
    "    np.mean([x[i] for x in all_Tmae_histories]) for i in range(num_epochs)]\n",
    "average_Tloss_history = [\n",
    "    np.mean([x[i] for x in all_Tloss_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save them to external text files so I can have them and print out whatever chart I need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/Full Smoothing/ValMae.txt', \"w\") as file:\n",
    "    file.write(str(average_mae_history))\n",
    "with open(Project_Path+ '/Results/Full Smoothing/ValLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_loss_history))\n",
    "with open(Project_Path+ '/Results/Full Smoothing/TrainMae.txt', \"w\") as file:\n",
    "    file.write(str(average_Tmae_history))\n",
    "with open(Project_Path+ '/Results/Full Smoothing/TrainLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_Tloss_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the list containing the Validation mae to find the appropriate training season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/Full Smoothing/ValMae.txt', \"r\") as file:\n",
    "    average_mae_history = eval(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am printing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.6):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history,label='Smoothed Train set on Validation set')\n",
    "plt.title('Full Smoothing 4-Fold Cross Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Mae')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find what is the best season. That is, the one with the youngest Mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "Min_Mae=min(smooth_mae_history)\n",
    "Best_Epoch=smooth_mae_history.index(Min_Mae)\n",
    "Best_Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now knowing the best training time I train the model and check it with the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test.csv',sep=',',index_col=0)\n",
    "TestDF=Apply_Smoothing(test_data) #smoothing\n",
    "TrainDF=Apply_Smoothing(train_data) #smoothing\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "test_targets=TestDF['Real_Rate']\n",
    "test_data=TestDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "test_targets = Normalization(test_targets)\n",
    "train_targets = Normalization(train_targets)\n",
    "\n",
    "Target=test_targets\n",
    "Target.append(train_targets)\n",
    "Bias_Initial_Out=Target.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "\n",
    "train_data,test_data=Scaller(train_data,test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been training for 24 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = Residual_NN(Input_Shape=train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data,train_targets,callbacks=callback,\n",
    "          epochs=24, batch_size=16, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check with the smoothed test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score, test_mape_score = model.evaluate(test_data, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing On Train evaluate on non-smouthed validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in the paper, I also did experiments with a smoothed train-set but a non-smoothed validation/test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes the data set I created in the previous notebook and converts it into a format so that it can be a validation/test-set to test with the smoothed train set. It does not cause data augmentation or smoothing. It's pretty much the same as the first \"Prepare_Dataset\" function that formatted our data just adds a 'Time_Distance' attribute used in smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Test(Test): \n",
    "    Test['Time_Distance']=0\n",
    "    Test=Test.drop(['Time_Int'], axis=1)\n",
    "    a=Test['Slot_Timeint']\n",
    "    b=Test['Ilegality_Rate']\n",
    "    Test=Test.drop(['Slot_Timeint'], axis=1)\n",
    "    Test=Test.drop(['Ilegality_Rate'], axis=1)\n",
    "    Test.insert(9, \"Real_Time\", a, True)\n",
    "    Test.insert(10, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Test=pd.merge(Test, Final_Weather_Data, on='Key')\n",
    "    Test=Test.drop(['Key'], axis=1)\n",
    "    \n",
    "    Test=pd.merge(Test, Distance_Data, on='Slot_id')\n",
    "    Test=Test.drop(['Slot_id'], axis=1)\n",
    "    return Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below applies an exponential decay technique to the learning rate after epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the same process as I did previously, simply in each iteration I apply smoothing only to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "TrainDF=train_data\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 60\n",
    "all_mae_histories = []\n",
    "all_loss_histories = []\n",
    "all_Tmae_histories = []\n",
    "all_Tloss_histories = []\n",
    "\n",
    "for i in tqdm(range(0,k)):\n",
    "        print('processing fold #', i)\n",
    "        TrainDF=shuffle(TrainDF)\n",
    "        Train,Val = train_test_split(TrainDF,test_size=0.2,random_state=42)\n",
    "        \n",
    "      \n",
    "        Val=Prepare_Test(Val)\n",
    "        Train=Apply_Smoothing(Train)\n",
    "        \n",
    "        Final_Train_targets=Train['Real_Rate']\n",
    "        Final_Train_data=Train.drop(['Real_Rate'], axis=1)\n",
    "        \n",
    "        Final_val_targets=Val['Real_Rate']\n",
    "        Final_val_data=Val.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "        \n",
    "        Final_Train_data,Final_val_data=Scaller(Final_Train_data,Final_val_data)\n",
    "        \n",
    "        Final_Train_targets = Normalization(Final_Train_targets)\n",
    "        Final_val_targets = Normalization(Final_val_targets)\n",
    "        \n",
    "        Target=Final_Train_targets\n",
    "        Target.append(Final_val_targets)\n",
    "        Bias_Initial_Out=Target.mean()\n",
    "        Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "        \n",
    "        \n",
    "        # Build the Keras model (already compiled)\n",
    "        model = Residual_NN(Input_Shape=Final_Train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = model.fit(Final_Train_data, Final_Train_targets,\n",
    "                            validation_data=(Final_val_data, Final_val_targets),\n",
    "                            epochs=num_epochs,callbacks=callback,batch_size=16, verbose=1)\n",
    "        mae_history = history.history['val_MAE']\n",
    "        loss_history = history.history['val_loss']\n",
    "        Tmae_history = history.history['MAE']\n",
    "        Tloss_history = history.history['loss']\n",
    "        all_mae_histories.append(mae_history)\n",
    "        all_loss_histories.append(loss_history)\n",
    "        all_Tmae_histories.append(Tmae_history)\n",
    "        all_Tloss_histories.append(Tloss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I find the averages of the results per season from the four iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_loss_history = [\n",
    "    np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
    "average_Tmae_history = [\n",
    "    np.mean([x[i] for x in all_Tmae_histories]) for i in range(num_epochs)]\n",
    "average_Tloss_history = [\n",
    "    np.mean([x[i] for x in all_Tloss_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save them to external text files so I can have them and print out whatever chart I need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/Smooth On Train Real Test/ValMae.txt', \"w\") as file:\n",
    "    file.write(str(average_mae_history))\n",
    "with open(Project_Path+ '/Results/Smooth On Train Real Test/ValLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_loss_history))\n",
    "with open(Project_Path+ '/Results/Smooth On Train Real Test/TrainMae.txt', \"w\") as file:\n",
    "    file.write(str(average_Tmae_history))\n",
    "with open(Project_Path+ '/Results/Smooth On Train Real Test/TrainLoss.txt', \"w\") as file:\n",
    "    file.write(str(average_Tloss_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the list containing the Validation mae to find the appropriate training season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Results/Smooth On Train Real Test/ValLoss.txt', \"r\") as file:\n",
    "    average_mae_history = eval(file.readline())\n",
    "with open(Project_Path+ '/Results/Full Smoothing/ValLoss.txt', \"r\") as file:\n",
    "    average_mae_history2 = eval(file.readline())\n",
    "with open(Project_Path+ '/Results/No Smoothing/ValLoss.txt', \"r\") as file:\n",
    "    average_mae_history3 = eval(file.readline())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am printing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.4):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history3[0:])\n",
    "smooth_mae_history2 = smooth_curve(average_mae_history2[0:])\n",
    "smooth_mae_history3 = smooth_curve(average_mae_history[0:])\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history,label= 'Train Set on Validation Set' )\n",
    "plt.plot(range(1, len(smooth_mae_history3) + 1), smooth_mae_history3,label= 'Smoothed Train Set on Validation Set')\n",
    "plt.plot(range(1, len(smooth_mae_history2) + 1), smooth_mae_history2,label= 'Smoothed Train Set on Smoothed Validation Set')\n",
    "\n",
    "plt.title('Influence of Smoothing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Mae')\n",
    "plt.legend()\n",
    "#fig.savefig(Project_Path+ '/Data/Validation_RMSE.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find what is the best season. That is, the one with the youngest Mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "Min_Mae=min(smooth_mae_history)\n",
    "Best_Epoch=smooth_mae_history.index(Min_Mae)\n",
    "Best_Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now knowing the best training time I train the model and check it with the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test.csv',sep=',',index_col=0)\n",
    "TestDF=Prepare_Test(test_data) #No smoothing\n",
    "TrainDF=Apply_Smoothing(train_data) #Smoothing\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "test_targets=TestDF['Real_Rate']\n",
    "test_data=TestDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "test_targets = Normalization(test_targets)\n",
    "train_targets = Normalization(train_targets)\n",
    "\n",
    "Target=test_targets\n",
    "Target.append(train_targets)\n",
    "Bias_Initial_Out=Target.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "\n",
    "train_data,test_data=Scaller(train_data,test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been training for 26 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = Residual_NN(Input_Shape=train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data,train_targets,callbacks=callback,\n",
    "          epochs=26, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check with the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score, test_mape_score = model.evaluate(test_data, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(Project_Path+ '/DNN_Regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I print the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value=model.predict(test_data)\n",
    "true_value=test_targets\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(true_value, predicted_value, c='crimson')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "p1 = max(max(predicted_value), max(true_value))\n",
    "p2 = min(min(predicted_value), min(true_value))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 22:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legal_illegal=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "TrainDF=Apply_Smoothing(Legal_illegal) #smoothing\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "#train_targets = Normalization(train_targets)\n",
    "\n",
    "Target=train_targets\n",
    "Bias_Initial_Out=Target.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "\n",
    "Standar_Scaller = StandardScaler()\n",
    "train_data=Standar_Scaller.fit_transform(train_data)\n",
    "with open('Standar_Scaller.pkl', 'wb') as f:\n",
    "    pickle.dump(Standar_Scaller, f,  protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = Residual_NN(Input_Shape=train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data,train_targets,callbacks=callback,\n",
    "          epochs=24, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'finalized_model.sav'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m12",
   "language": "python",
   "name": "tf_m12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
